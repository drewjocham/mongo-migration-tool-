package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"
	"time"

	"github.com/anthropics/anthropic-sdk-go"
	anthropic_option "github.com/anthropics/anthropic-sdk-go/option"
	"github.com/google/generative-ai-go/genai"
	"github.com/sashabaranov/go-openai"
	"github.com/spf13/cobra"
	"go.mongodb.org/mongo-driver/bson"
	"go.mongodb.org/mongo-driver/mongo/options"
	"google.golang.org/api/docs/v1"
	"google.golang.org/api/drive/v3"
	"google.golang.org/api/option"

	"github.com/jocham/mongo-essential/internal/config"
)

var (
	analyzeCollectionFlag string
	analyzeFormatFlag     string
	analyzeDetailFlag     bool
	analyzeProviderFlag   string
	googleDocsFlag        bool
	googleDocsTitle       string
	googleDocsShare       string
	googleDocsFolderID    string
)

// AIProvider interface for different AI services
type AIProvider interface {
	Analyze(context.Context, string) (string, error)
	GetProviderName() string
}

// OpenAIProvider implements AIProvider for OpenAI
type OpenAIProvider struct {
	client *openai.Client
	model  string
}

// GeminiProvider implements AIProvider for Google Gemini
type GeminiProvider struct {
	client *genai.Client
	model  string
}

// ClaudeProvider implements AIProvider for Anthropic Claude
// TODO: Re-implement Claude support
type ClaudeProvider struct {
	// client *anthropic.Client
	model string
}

// DatabaseAnalysis represents the structure of database analysis data
type DatabaseAnalysis struct {
	DatabaseInfo    DatabaseInfo     `json:"database_info"`
	Collections     []CollectionInfo `json:"collections"`
	Indexes         []IndexInfo      `json:"indexes"`
	Performance     PerformanceInfo  `json:"performance"`
	Recommendations []string         `json:"recommendations,omitempty"`
}

type DatabaseInfo struct {
	Name            string                 `json:"name"`
	SizeOnDisk      int64                  `json:"size_on_disk"`
	CollectionCount int                    `json:"collection_count"`
	IndexCount      int                    `json:"index_count"`
	DataSize        int64                  `json:"data_size"`
	StorageSize     int64                  `json:"storage_size"`
	Stats           map[string]interface{} `json:"stats"`
}

type CollectionInfo struct {
	Name            string                 `json:"name"`
	Count           int64                  `json:"count"`
	Size            int64                  `json:"size"`
	AvgObjSize      int64                  `json:"avg_obj_size"`
	StorageSize     int64                  `json:"storage_size"`
	IndexCount      int                    `json:"index_count"`
	SampleDoc       map[string]interface{} `json:"sample_doc,omitempty"`
	ValidationRules map[string]interface{} `json:"validation_rules,omitempty"`
}

type IndexInfo struct {
	Collection string                 `json:"collection"`
	Name       string                 `json:"name"`
	Keys       map[string]interface{} `json:"keys"`
	Size       int64                  `json:"size"`
	Usage      map[string]interface{} `json:"usage,omitempty"`
	Unique     bool                   `json:"unique"`
	Sparse     bool                   `json:"sparse"`
	TTL        *time.Duration         `json:"ttl,omitempty"`
}

type PerformanceInfo struct {
	SlowQueries     []SlowQuery            `json:"slow_queries,omitempty"`
	IndexUsage      map[string]interface{} `json:"index_usage"`
	ConnectionStats map[string]interface{} `json:"connection_stats"`
	OpCounters      map[string]interface{} `json:"op_counters"`
	OplogInfo       *OplogInfo             `json:"oplog_info,omitempty"`
	ChangeStreams   []ChangeStreamInfo     `json:"change_streams,omitempty"`
	ReplicationInfo *ReplicationInfo       `json:"replication_info,omitempty"`
}

type SlowQuery struct {
	Timestamp  time.Time              `json:"timestamp"`
	Duration   time.Duration          `json:"duration"`
	Command    map[string]interface{} `json:"command"`
	Collection string                 `json:"collection"`
}

type OplogInfo struct {
	Size           int64            `json:"size"`
	UsedSize       int64            `json:"used_size"`
	MaxSize        int64            `json:"max_size"`
	EntryCount     int64            `json:"entry_count"`
	FirstEntry     *OplogEntry      `json:"first_entry,omitempty"`
	LastEntry      *OplogEntry      `json:"last_entry,omitempty"`
	TimeWindow     time.Duration    `json:"time_window"`
	OperationTypes map[string]int64 `json:"operation_types"`
	Namespaces     map[string]int64 `json:"namespaces"`
	Throughput     OplogThroughput  `json:"throughput"`
}

type OplogEntry struct {
	Timestamp time.Time              `json:"timestamp"`
	Operation string                 `json:"operation"`
	Namespace string                 `json:"namespace"`
	WallTime  *time.Time             `json:"wall_time,omitempty"`
	Document  map[string]interface{} `json:"document,omitempty"`
}

type OplogThroughput struct {
	OpsPerSecond   float64 `json:"ops_per_second"`
	BytesPerSecond float64 `json:"bytes_per_second"`
	PeakOpsPerSec  float64 `json:"peak_ops_per_sec"`
	AvgOpsPerSec   float64 `json:"avg_ops_per_sec"`
}

type ChangeStreamInfo struct {
	Namespace      string                   `json:"namespace"`
	ResumeToken    map[string]interface{}   `json:"resume_token,omitempty"`
	Watched        bool                     `json:"watched"`
	OperationTypes []string                 `json:"operation_types"`
	Pipeline       []map[string]interface{} `json:"pipeline,omitempty"`
	FullDocument   string                   `json:"full_document,omitempty"`
	StartTime      *time.Time               `json:"start_time,omitempty"`
}

type ReplicationInfo struct {
	IsReplicaSet   bool                     `json:"is_replica_set"`
	IsMaster       bool                     `json:"is_master"`
	IsSecondary    bool                     `json:"is_secondary"`
	ReplicaSetName string                   `json:"replica_set_name,omitempty"`
	Members        []ReplicaSetMember       `json:"members,omitempty"`
	ReplicationLag map[string]time.Duration `json:"replication_lag,omitempty"`
	OplogWindow    time.Duration            `json:"oplog_window"`
	SyncSource     string                   `json:"sync_source,omitempty"`
	ElectionId     map[string]interface{}   `json:"election_id,omitempty"`
}

type ReplicaSetMember struct {
	Host       string        `json:"host"`
	State      int           `json:"state"`
	StateStr   string        `json:"state_str"`
	Health     int           `json:"health"`
	Optime     *time.Time    `json:"optime,omitempty"`
	OptimeDate *time.Time    `json:"optime_date,omitempty"`
	Lag        time.Duration `json:"lag"`
	IsSelf     bool          `json:"is_self"`
	Priority   float64       `json:"priority"`
}

// GoogleDocsExporter handles Google Docs integration
type GoogleDocsExporter struct {
	docsService  *docs.Service
	driveService *drive.Service
	config       *config.Config
}

// AnalysisReport represents a complete analysis report
type AnalysisReport struct {
	Title             string            `json:"title"`
	GeneratedAt       time.Time         `json:"generated_at"`
	Provider          string            `json:"provider"`
	DatabaseName      string            `json:"database_name"`
	AnalysisType      string            `json:"analysis_type"`
	DatabaseAnalysis  *DatabaseAnalysis `json:"database_analysis,omitempty"`
	AIRecommendations string            `json:"ai_recommendations"`
	ExecutionTime     time.Duration     `json:"execution_time"`
	DocumentID        string            `json:"document_id,omitempty"`
	DocumentURL       string            `json:"document_url,omitempty"`
}

// aiCmd represents the AI analysis command
var aiCmd = &cobra.Command{
	Use:   "ai",
	Short: "AI-powered database analysis",
	Long: `AI-powered MongoDB database analysis using OpenAI, Gemini, or Claude.

This command analyzes your MongoDB database and provides intelligent insights
about performance, schema design, indexing strategies, and optimization opportunities.

Supported AI providers:
- OpenAI (GPT-4, GPT-3.5)
- Google Gemini
- Anthropic Claude`,
}

// analyzeCmd analyzes the database using AI
var analyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze database with AI assistance",
	Long: `Perform comprehensive database analysis using AI.
	
The analysis includes:
- Collection schema analysis
- Index usage and optimization recommendations  
- Performance insights
- Query optimization suggestions
- Oplog and replication health analysis
- Change stream configuration review
- Security recommendations
- Best practices compliance

Examples:
  mongo-essential ai analyze --provider openai
  mongo-essential ai analyze --collection users --detail
  mongo-essential ai analyze --format json --provider gemini
	RunE: func(cmd *cobra.Command, args []string) error {
		return runDatabaseAnalysis(cmd.Context())
	},
}

// schemaCmd analyzes collection schemas
var schemaCmd = &cobra.Command{
	Use:   "schema [collection]",
	Short: "Analyze collection schemas with AI",
	Long: `Analyze MongoDB collection schemas and provide recommendations.

This command examines collection structures, data types, field patterns,
and provides suggestions for schema optimization and normalization.`,
	RunE: func(cmd *cobra.Command, args []string) error {
		collection := ""
		if len(args) > 0 {
			collection = args[0]
		}
		return runSchemaAnalysis(cmd.Context(), collection)
	},
}

// performanceCmd analyzes database performance
var performanceCmd = &cobra.Command{
	Use:   "performance",
	Short: "Analyze database performance with AI",
	Long: `Analyze MongoDB performance metrics and provide optimization recommendations.

This includes slow query analysis, index utilization, connection patterns,
and resource usage optimization suggestions.`,
	RunE: func(cmd *cobra.Command, args []string) error {
		return runPerformanceAnalysis(cmd.Context())
	},
}

// oplogCmd analyzes oplog and replication
var oplogCmd = &cobra.Command{
	Use:   "oplog",
	Short: "Analyze oplog and replication with AI",
	Long: `Analyze MongoDB oplog, replication health, and provide recommendations.

This includes oplog size analysis, replication lag detection, throughput analysis,
operation pattern insights, and replica set health assessment.`,
	RunE: func(cmd *cobra.Command, args []string) error {
		return runOplogAnalysis(cmd.Context())
	},
}

// changestreamCmd analyzes change streams
var changestreamCmd = &cobra.Command{
	Use:   "changestream",
	Short: "Analyze change streams with AI",
	Long: `Analyze MongoDB change streams and provide optimization recommendations.

This includes change stream configuration analysis, resume token management,
performance optimization, and real-time processing patterns.`,
	RunE: func(cmd *cobra.Command, args []string) error {
		return runChangeStreamAnalysis(cmd.Context())
	},
}

func init() {
	// Add ai command to root
	rootCmd.AddCommand(aiCmd)

	// Add subcommands
	aiCmd.AddCommand(analyzeCmd)
	aiCmd.AddCommand(schemaCmd)
	aiCmd.AddCommand(performanceCmd)
	aiCmd.AddCommand(oplogCmd)
	aiCmd.AddCommand(changestreamCmd)

	// Flags for analyze command
	analyzeCmd.Flags().StringVar(&analyzeCollectionFlag, "collection", "", "Analyze specific collection")
	analyzeCmd.Flags().StringVar(&analyzeFormatFlag, "format", "text", "Output format (text, json)")
	analyzeCmd.Flags().BoolVar(&analyzeDetailFlag, "detail", false, "Include detailed analysis")
	analyzeCmd.Flags().StringVar(&analyzeProviderFlag, "provider", "", "AI provider to use (openai, gemini, claude)")

	// Flags for schema command
	schemaCmd.Flags().StringVar(&analyzeFormatFlag, "format", "text", "Output format (text, json)")
	schemaCmd.Flags().BoolVar(&analyzeDetailFlag, "detail", false, "Include detailed schema analysis")
	schemaCmd.Flags().StringVar(&analyzeProviderFlag, "provider", "", "AI provider to use")

	// Flags for performance command
	performanceCmd.Flags().StringVar(&analyzeFormatFlag, "format", "text", "Output format (text, json)")
	performanceCmd.Flags().StringVar(&analyzeProviderFlag, "provider", "", "AI provider to use")

	// Flags for oplog command
	oplogCmd.Flags().StringVar(&analyzeFormatFlag, "format", "text", "Output format (text, json)")
	oplogCmd.Flags().StringVar(&analyzeProviderFlag, "provider", "", "AI provider to use")
	oplogCmd.Flags().BoolVar(&analyzeDetailFlag, "detail", false, "Include detailed oplog analysis")

	// Flags for changestream command
	changestreamCmd.Flags().StringVar(&analyzeFormatFlag, "format", "text", "Output format (text, json)")
	changestreamCmd.Flags().StringVar(&analyzeProviderFlag, "provider", "", "AI provider to use")
	changestreamCmd.Flags().StringVar(&analyzeCollectionFlag, "collection", "", "Analyze change streams for specific collection")

	// Global flags
	aiCmd.PersistentFlags().BoolVar(&analyzeDetailFlag, "detail", false, "Include detailed analysis")
	aiCmd.PersistentFlags().StringVar(&analyzeProviderFlag, "provider", "", "AI provider (openai, gemini, claude)")

	// Google Docs integration flags
	aiCmd.PersistentFlags().BoolVar(&googleDocsFlag, "google-docs", false, "Export report to Google Docs")
	aiCmd.PersistentFlags().StringVar(&googleDocsTitle, "docs-title", "", "Custom title for Google Docs report")
	aiCmd.PersistentFlags().StringVar(&googleDocsShare, "docs-share", "", "Email address to share the document with")
	aiCmd.PersistentFlags().StringVar(&googleDocsFolderID, "docs-folder", "", "Google Drive folder ID to store the document")
}

// Implementation of AI providers

func (p *OpenAIProvider) Analyze(ctx context.Context, prompt string) (string, error) {
	resp, err := p.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
		Model: p.model,
		Messages: []openai.ChatCompletionMessage{
			{
				Role:    openai.ChatMessageRoleSystem,
				Content: "You are a MongoDB database expert. Analyze the provided database information and provide detailed insights, recommendations, and optimizations. Focus on performance, indexing, schema design, and best practices.",
			},
			{
				Role:    openai.ChatMessageRoleUser,
				Content: prompt,
			},
		},
		MaxTokens:   2000,
		Temperature: 0.3,
	})

	if err != nil {
		return "", fmt.Errorf("OpenAI API error: %w", err)
	}

	if len(resp.Choices) == 0 {
		return "", fmt.Errorf("no response from OpenAI")
	}

	return resp.Choices[0].Message.Content, nil
}

func (p *OpenAIProvider) GetProviderName() string {
	return "OpenAI " + p.model
}

func (p *GeminiProvider) Analyze(ctx context.Context, prompt string) (string, error) {
	model := p.client.GenerativeModel(p.model)
	model.SetTemperature(0.3)
	model.SetMaxOutputTokens(2000)

	systemPrompt := "You are a MongoDB database expert. Analyze the provided database information and provide detailed insights, recommendations, and optimizations. Focus on performance, indexing, schema design, and best practices."
	fullPrompt := systemPrompt + "\n\nDatabase Analysis Data:\n" + prompt

	resp, err := model.GenerateContent(ctx, genai.Text(fullPrompt))
	if err != nil {
		return "", fmt.Errorf("Gemini API error: %w", err)
	}

	if len(resp.Candidates) == 0 || len(resp.Candidates[0].Content.Parts) == 0 {
		return "", fmt.Errorf("no response from Gemini")
	}

	return fmt.Sprintf("%v", resp.Candidates[0].Content.Parts[0]), nil
}

func (p *GeminiProvider) GetProviderName() string {
	return "Google " + p.model
}

func (p *ClaudeProvider) Analyze(ctx context.Context, prompt string) (string, error) {
	// TODO: Implement Claude API integration
	return "", fmt.Errorf("Claude provider not implemented yet")
}

func (p *ClaudeProvider) GetProviderName() string {
	return "Anthropic " + p.model
}

// Helper functions

func createAIProvider(config *config.Config) (AIProvider, error) {
	provider := analyzeProviderFlag
	if provider == "" {
		provider = config.AIProvider
	}

	if !config.AIEnabled {
		return nil, fmt.Errorf("AI analysis is disabled. Set AI_ENABLED=true in your configuration")
	}

	switch strings.ToLower(provider) {
	case "openai":
		if config.OpenAIAPIKey == "" {
			return nil, fmt.Errorf("OpenAI API key not configured. Set OPENAI_API_KEY environment variable")
		}
		return &OpenAIProvider{
			client: openai.NewClient(config.OpenAIAPIKey),
			model:  config.OpenAIModel,
		}, nil

	case "gemini":
		if config.GeminiAPIKey == "" {
			return nil, fmt.Errorf("Gemini API key not configured. Set GEMINI_API_KEY environment variable")
		}
		client, err := genai.NewClient(context.Background(), option.WithAPIKey(config.GeminiAPIKey))
		if err != nil {
			return nil, fmt.Errorf("failed to create Gemini client: %w", err)
		}
		return &GeminiProvider{
			client: client,
			model:  config.GeminiModel,
		}, nil

	case "claude":
		// TODO: Re-implement Claude support
		return nil, fmt.Errorf("Claude provider temporarily disabled - use openai or gemini")

	default:
		return nil, fmt.Errorf("unsupported AI provider: %s. Supported providers: openai, gemini (claude coming soon)", provider)
	}
}

func runDatabaseAnalysis(ctx context.Context) error {
	startTime := time.Now()
	fmt.Println("üîç Starting AI-powered database analysis...")
	fmt.Println(strings.Repeat("=", 60))

	// Create AI provider
	aiProvider, err := createAIProvider(cfg)
	if err != nil {
		return fmt.Errorf("failed to create AI provider: %w", err)
	}

	fmt.Printf("Using AI Provider: %s\n\n", aiProvider.GetProviderName())

	// Collect database information
	analysis, err := collectDatabaseInfo(ctx)
	if err != nil {
		return fmt.Errorf("failed to collect database information: %w", err)
	}

	// Convert to JSON for AI analysis
	analysisJSON, err := json.MarshalIndent(analysis, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal analysis data: %w", err)
	}

	// Create analysis prompt
	prompt := fmt.Sprintf(`Please analyze this MongoDB database and provide comprehensive recommendations:

Database Analysis Data:
%s

Please provide:
1. Overall database health assessment
2. Performance optimization recommendations
3. Index optimization suggestions
4. Schema design improvements
5. Security considerations
6. Best practices compliance
7. Specific action items

Format the response with clear sections and actionable recommendations.`, string(analysisJSON))

	// Get AI analysis
	fmt.Println("ü§ñ Analyzing database with AI...")
	aiResponse, err := aiProvider.Analyze(ctx, prompt)
	if err != nil {
		return fmt.Errorf("AI analysis failed: %w", err)
	}

	// Create analysis report
	report := &AnalysisReport{
		Title:             generateReportTitle("Database Analysis", cfg.Database),
		GeneratedAt:       time.Now().UTC(),
		Provider:          aiProvider.GetProviderName(),
		DatabaseName:      cfg.Database,
		AnalysisType:      "comprehensive",
		DatabaseAnalysis:  analysis,
		AIRecommendations: aiResponse,
		ExecutionTime:     time.Since(startTime),
	}

	// Export to Google Docs if requested
	if googleDocsFlag {
		if err := exportToGoogleDocs(ctx, report); err != nil {
			fmt.Printf("‚ö†Ô∏è  Google Docs export failed: %v\n", err)
			fmt.Println("Continuing with local output...")
		} else {
			fmt.Printf("‚úì Report exported to Google Docs: %s\n", report.DocumentURL)
		}
	}

	// Output results locally
	if analyzeFormatFlag == "json" {
		jsonOutput, _ := json.MarshalIndent(report, "", "  ")
		fmt.Println(string(jsonOutput))
	} else {
		fmt.Println("\n" + strings.Repeat("=", 60))
		fmt.Printf("AI Analysis Results (%s)\n", aiProvider.GetProviderName())
		fmt.Println(strings.Repeat("=", 60))
		fmt.Println(aiResponse)
		if report.DocumentURL != "" {
			fmt.Printf("\nüìÑ Google Docs: %s\n", report.DocumentURL)
		}
	}

	return nil
}

func runSchemaAnalysis(ctx context.Context, collection string) error {
	fmt.Println("üîç Starting AI-powered schema analysis...")

	aiProvider, err := createAIProvider(cfg)
	if err != nil {
		return err
	}

	collections := []string{}
	if collection != "" {
		collections = []string{collection}
	} else {
		// Get all collections
		colls, err := db.ListCollectionNames(ctx, bson.M{})
		if err != nil {
			return fmt.Errorf("failed to list collections: %w", err)
		}
		collections = colls
	}

	var schemaData []CollectionInfo
	for _, coll := range collections {
		info, err := analyzeCollectionSchema(ctx, coll)
		if err != nil {
			fmt.Printf("Warning: failed to analyze collection %s: %v\n", coll, err)
			continue
		}
		schemaData = append(schemaData, info)
	}

	schemaJSON, _ := json.MarshalIndent(schemaData, "", "  ")
	prompt := fmt.Sprintf(`Analyze these MongoDB collection schemas and provide recommendations:

Schema Data:
%s

Please provide:
1. Schema design assessment
2. Data type optimization suggestions
3. Normalization/denormalization recommendations
4. Field naming and structure improvements
5. Validation rule suggestions
6. Migration strategies if needed

Format with clear sections and specific recommendations.`, string(schemaJSON))

	aiResponse, err := aiProvider.Analyze(ctx, prompt)
	if err != nil {
		return err
	}

	fmt.Printf("\nü§ñ Schema Analysis (%s)\n", aiProvider.GetProviderName())
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println(aiResponse)

	return nil
}

func runPerformanceAnalysis(ctx context.Context) error {
	fmt.Println("üîç Starting AI-powered performance analysis...")

	aiProvider, err := createAIProvider(cfg)
	if err != nil {
		return err
	}

	perfInfo, err := collectPerformanceInfo(ctx)
	if err != nil {
		return fmt.Errorf("failed to collect performance information: %w", err)
	}

	perfJSON, _ := json.MarshalIndent(perfInfo, "", "  ")
	prompt := fmt.Sprintf(`Analyze this MongoDB performance data and provide optimization recommendations:

Performance Data:
%s

Please provide:
1. Performance bottleneck identification
2. Index optimization recommendations
3. Query optimization suggestions
4. Resource utilization analysis
5. Scaling recommendations
6. Monitoring suggestions

Focus on actionable improvements with specific implementation steps.`, string(perfJSON))

	aiResponse, err := aiProvider.Analyze(ctx, prompt)
	if err != nil {
		return err
	}

	fmt.Printf("\nü§ñ Performance Analysis (%s)\n", aiProvider.GetProviderName())
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println(aiResponse)

	return nil
}

// Database information collection functions

func collectDatabaseInfo(ctx context.Context) (*DatabaseAnalysis, error) {
	analysis := &DatabaseAnalysis{}

	// Database info
	dbStats := db.RunCommand(ctx, bson.M{"dbStats": 1, "scale": 1})
	if dbStats.Err() != nil {
		return nil, fmt.Errorf("failed to get database stats: %w", dbStats.Err())
	}

	var stats map[string]interface{}
	if err := dbStats.Decode(&stats); err != nil {
		return nil, fmt.Errorf("failed to decode database stats: %w", err)
	}

	analysis.DatabaseInfo = DatabaseInfo{
		Name:            cfg.Database,
		Stats:           stats,
		SizeOnDisk:      int64(stats["fsUsedSize"].(float64)),
		DataSize:        int64(stats["dataSize"].(float64)),
		StorageSize:     int64(stats["storageSize"].(float64)),
		CollectionCount: int(stats["collections"].(float64)),
		IndexCount:      int(stats["indexes"].(float64)),
	}

	// Collection info
	collections, err := collectCollectionInfo(ctx)
	if err != nil {
		return nil, err
	}
	analysis.Collections = collections

	// Index info
	indexes, err := collectIndexInfo(ctx)
	if err != nil {
		return nil, err
	}
	analysis.Indexes = indexes

	// Performance info
	perfInfo, err := collectPerformanceInfo(ctx)
	if err != nil {
		return nil, err
	}
	analysis.Performance = *perfInfo

	return analysis, nil
}

func collectCollectionInfo(ctx context.Context) ([]CollectionInfo, error) {
	collections, err := db.ListCollectionNames(ctx, bson.M{})
	if err != nil {
		return nil, err
	}

	var collectionInfos []CollectionInfo
	for _, collName := range collections {
		info, err := analyzeCollectionSchema(ctx, collName)
		if err != nil {
			continue // Skip collections we can't analyze
		}
		collectionInfos = append(collectionInfos, info)
	}

	return collectionInfos, nil
}

func analyzeCollectionSchema(ctx context.Context, collectionName string) (CollectionInfo, error) {
	coll := db.Collection(collectionName)

	// Get collection stats
	statsResult := db.RunCommand(ctx, bson.M{"collStats": collectionName})
	var stats map[string]interface{}
	statsResult.Decode(&stats)

	// Get document count
	count, _ := coll.CountDocuments(ctx, bson.M{})

	// Get sample document
	cursor, _ := coll.Find(ctx, bson.M{}, options.Find().SetLimit(1))
	var sampleDoc map[string]interface{}
	if cursor.Next(ctx) {
		cursor.Decode(&sampleDoc)
	}
	cursor.Close(ctx)

	// Get indexes count
	indexCursor, _ := coll.Indexes().List(ctx)
	indexCount := 0
	for indexCursor.Next(ctx) {
		indexCount++
	}
	indexCursor.Close(ctx)

	info := CollectionInfo{
		Name:       collectionName,
		Count:      count,
		IndexCount: indexCount,
		SampleDoc:  sampleDoc,
	}

	if stats != nil {
		if size, ok := stats["size"].(float64); ok {
			info.Size = int64(size)
		}
		if storageSize, ok := stats["storageSize"].(float64); ok {
			info.StorageSize = int64(storageSize)
		}
		if avgObjSize, ok := stats["avgObjSize"].(float64); ok {
			info.AvgObjSize = int64(avgObjSize)
		}
	}

	return info, nil
}

func collectIndexInfo(ctx context.Context) ([]IndexInfo, error) {
	collections, err := db.ListCollectionNames(ctx, bson.M{})
	if err != nil {
		return nil, err
	}

	var allIndexes []IndexInfo
	for _, collName := range collections {
		coll := db.Collection(collName)
		cursor, err := coll.Indexes().List(ctx)
		if err != nil {
			continue
		}

		for cursor.Next(ctx) {
			var indexSpec map[string]interface{}
			cursor.Decode(&indexSpec)

			indexInfo := IndexInfo{
				Collection: collName,
				Name:       indexSpec["name"].(string),
				Keys:       indexSpec["key"].(map[string]interface{}),
			}

			if unique, ok := indexSpec["unique"].(bool); ok {
				indexInfo.Unique = unique
			}

			if sparse, ok := indexSpec["sparse"].(bool); ok {
				indexInfo.Sparse = sparse
			}

			allIndexes = append(allIndexes, indexInfo)
		}
		cursor.Close(ctx)
	}

	return allIndexes, nil
}

func collectPerformanceInfo(ctx context.Context) (*PerformanceInfo, error) {
	perfInfo := &PerformanceInfo{
		IndexUsage:      make(map[string]interface{}),
		ConnectionStats: make(map[string]interface{}),
		OpCounters:      make(map[string]interface{}),
	}

	// Get server status
	serverStatusResult := db.RunCommand(ctx, bson.M{"serverStatus": 1})
	var serverStatus map[string]interface{}
	if serverStatusResult.Err() == nil {
		serverStatusResult.Decode(&serverStatus)

		if opcounters, ok := serverStatus["opcounters"].(map[string]interface{}); ok {
			perfInfo.OpCounters = opcounters
		}

		if connections, ok := serverStatus["connections"].(map[string]interface{}); ok {
			perfInfo.ConnectionStats = connections
		}
	}

	// Collect oplog information
	if oplogInfo, err := collectOplogInfo(ctx); err == nil {
		perfInfo.OplogInfo = oplogInfo
	}

	// Collect replication information
	if replInfo, err := collectReplicationInfo(ctx); err == nil {
		perfInfo.ReplicationInfo = replInfo
	}

	// Collect change stream information
	if changeStreams, err := collectChangeStreamInfo(ctx); err == nil {
		perfInfo.ChangeStreams = changeStreams
	}

	return perfInfo, nil
}

// Google Docs Integration Functions

func generateReportTitle(analysisType, databaseName string) string {
	if googleDocsTitle != "" {
		return googleDocsTitle
	}
	timestamp := time.Now().Format("2006-01-02 15:04")
	return fmt.Sprintf("%s Report - %s (%s)", analysisType, databaseName, timestamp)
}

func exportToGoogleDocs(ctx context.Context, report *AnalysisReport) error {
	if !cfg.GoogleDocsEnabled {
		return fmt.Errorf("Google Docs integration is disabled. Set GOOGLE_DOCS_ENABLED=true")
	}

	exporter, err := createGoogleDocsExporter(ctx)
	if err != nil {
		return fmt.Errorf("failed to create Google Docs exporter: %w", err)
	}

	return exporter.ExportReport(ctx, report)
}

func createGoogleDocsExporter(ctx context.Context) (*GoogleDocsExporter, error) {
	var creds option.ClientOption

	// Try credentials file first, then JSON
	if cfg.GoogleCredentialsPath != "" {
		creds = option.WithCredentialsFile(cfg.GoogleCredentialsPath)
	} else if cfg.GoogleCredentialsJSON != "" {
		creds = option.WithCredentialsJSON([]byte(cfg.GoogleCredentialsJSON))
	} else {
		return nil, fmt.Errorf("no Google credentials provided. Set GOOGLE_CREDENTIALS_PATH or GOOGLE_CREDENTIALS_JSON")
	}

	docsService, err := docs.NewService(ctx, creds)
	if err != nil {
		return nil, fmt.Errorf("failed to create Docs service: %w", err)
	}

	driveService, err := drive.NewService(ctx, creds)
	if err != nil {
		return nil, fmt.Errorf("failed to create Drive service: %w", err)
	}

	return &GoogleDocsExporter{
		docsService:  docsService,
		driveService: driveService,
		config:       cfg,
	}, nil
}

func (g *GoogleDocsExporter) ExportReport(ctx context.Context, report *AnalysisReport) error {
	// Create the document
	docRequest := &docs.Document{
		Title: report.Title,
	}

	doc, err := g.docsService.Documents.Create(docRequest).Context(ctx).Do()
	if err != nil {
		return fmt.Errorf("failed to create document: %w", err)
	}

	report.DocumentID = doc.DocumentId
	report.DocumentURL = fmt.Sprintf("https://docs.google.com/document/d/%s/edit", doc.DocumentId)

	// Move to specified folder if provided
	folderID := googleDocsFolderID
	if folderID == "" {
		folderID = g.config.GoogleDriveFolderID
	}
	if folderID != "" {
		if err := g.moveToFolder(ctx, doc.DocumentId, folderID); err != nil {
			fmt.Printf("Warning: failed to move document to folder: %v\n", err)
		}
	}

	// Add content to the document
	if err := g.addContentToDocument(ctx, doc.DocumentId, report); err != nil {
		return fmt.Errorf("failed to add content to document: %w", err)
	}

	// Share the document if email is provided
	shareEmail := googleDocsShare
	if shareEmail == "" {
		shareEmail = g.config.GoogleDocsShareWithEmail
	}
	if shareEmail != "" {
		if err := g.shareDocument(ctx, doc.DocumentId, shareEmail); err != nil {
			fmt.Printf("Warning: failed to share document: %v\n", err)
		}
	}

	return nil
}

func (g *GoogleDocsExporter) moveToFolder(ctx context.Context, documentID, folderID string) error {
	// Get current parents
	file, err := g.driveService.Files.Get(documentID).Fields("parents").Context(ctx).Do()
	if err != nil {
		return err
	}

	// Move to new folder
	_, err = g.driveService.Files.Update(documentID, nil).
		AddParents(folderID).
		RemoveParents(strings.Join(file.Parents, ",")).
		Context(ctx).Do()

	return err
}

func (g *GoogleDocsExporter) shareDocument(ctx context.Context, documentID, email string) error {
	permission := &drive.Permission{
		Type:         "user",
		Role:         "writer",
		EmailAddress: email,
	}

	_, err := g.driveService.Permissions.Create(documentID, permission).
		Context(ctx).Do()

	return err
}

func (g *GoogleDocsExporter) addContentToDocument(ctx context.Context, documentID string, report *AnalysisReport) error {
	requests := []*docs.Request{}

	// Add title and header
	requests = append(requests, g.createHeaderRequests(report)...)

	// Add database overview
	if report.DatabaseAnalysis != nil {
		requests = append(requests, g.createDatabaseOverviewRequests(report.DatabaseAnalysis)...)
	}

	// Add AI recommendations
	requests = append(requests, g.createAIRecommendationsRequests(report.AIRecommendations)...)

	// Add collection details
	if report.DatabaseAnalysis != nil && len(report.DatabaseAnalysis.Collections) > 0 {
		requests = append(requests, g.createCollectionDetailsRequests(report.DatabaseAnalysis.Collections)...)
	}

	// Add index analysis
	if report.DatabaseAnalysis != nil && len(report.DatabaseAnalysis.Indexes) > 0 {
		requests = append(requests, g.createIndexAnalysisRequests(report.DatabaseAnalysis.Indexes)...)
	}

	// Add performance metrics
	if report.DatabaseAnalysis != nil {
		requests = append(requests, g.createPerformanceRequests(&report.DatabaseAnalysis.Performance)...)
	}

	// Execute batch update
	batchRequest := &docs.BatchUpdateDocumentRequest{
		Requests: requests,
	}

	_, err := g.docsService.Documents.BatchUpdate(documentID, batchRequest).Context(ctx).Do()
	return err
}

func (g *GoogleDocsExporter) createHeaderRequests(report *AnalysisReport) []*docs.Request {
	headerText := fmt.Sprintf("%s\n\nGenerated: %s\nProvider: %s\nExecution Time: %v\n\n",
		report.Title,
		report.GeneratedAt.Format("2006-01-02 15:04:05 UTC"),
		report.Provider,
		report.ExecutionTime,
	)

	return []*docs.Request{
		{
			InsertText: &docs.InsertTextRequest{
				Location: &docs.Location{Index: 1},
				Text:     headerText,
			},
		},
	}
}

func (g *GoogleDocsExporter) createDatabaseOverviewRequests(analysis *DatabaseAnalysis) []*docs.Request {
	text := fmt.Sprintf("Database Overview\n\nDatabase: %s\nCollections: %d\nIndexes: %d\nData Size: %s\nStorage Size: %s\n\n",
		analysis.DatabaseInfo.Name,
		analysis.DatabaseInfo.CollectionCount,
		analysis.DatabaseInfo.IndexCount,
		formatBytes(analysis.DatabaseInfo.DataSize),
		formatBytes(analysis.DatabaseInfo.StorageSize),
	)

	return []*docs.Request{
		{
			InsertText: &docs.InsertTextRequest{
				Location: &docs.Location{Index: -1},
				Text:     text,
			},
		},
	}
}

func (g *GoogleDocsExporter) createAIRecommendationsRequests(recommendations string) []*docs.Request {
	text := fmt.Sprintf("AI Analysis & Recommendations\n\n%s\n\n", recommendations)

	return []*docs.Request{
		{
			InsertText: &docs.InsertTextRequest{
				Location: &docs.Location{Index: -1},
				Text:     text,
			},
		},
	}
}

func (g *GoogleDocsExporter) createCollectionDetailsRequests(collections []CollectionInfo) []*docs.Request {
	text := "Collection Details\n\n"
	for _, coll := range collections {
		text += fmt.Sprintf("‚Ä¢ %s: %d documents, %d indexes, %s storage\n",
			coll.Name, coll.Count, coll.IndexCount, formatBytes(coll.StorageSize))
	}
	text += "\n"

	return []*docs.Request{
		{
			InsertText: &docs.InsertTextRequest{
				Location: &docs.Location{Index: -1},
				Text:     text,
			},
		},
	}
}

func (g *GoogleDocsExporter) createIndexAnalysisRequests(indexes []IndexInfo) []*docs.Request {
	text := "Index Analysis\n\n"
	for _, idx := range indexes {
		text += fmt.Sprintf("‚Ä¢ %s.%s: %v", idx.Collection, idx.Name, idx.Keys)
		if idx.Unique {
			text += " (UNIQUE)"
		}
		if idx.Sparse {
			text += " (SPARSE)"
		}
		text += "\n"
	}
	text += "\n"

	return []*docs.Request{
		{
			InsertText: &docs.InsertTextRequest{
				Location: &docs.Location{Index: -1},
				Text:     text,
			},
		},
	}
}

func (g *GoogleDocsExporter) createPerformanceRequests(perf *PerformanceInfo) []*docs.Request {
	text := "Performance Metrics\n\n"

	if len(perf.OpCounters) > 0 {
		text += "Operation Counters:\n"
		for op, count := range perf.OpCounters {
			text += fmt.Sprintf("‚Ä¢ %s: %v\n", op, count)
		}
		text += "\n"
	}

	if perf.OplogInfo != nil {
		text += fmt.Sprintf("Oplog Information:\n‚Ä¢ Size: %s\n‚Ä¢ Time Window: %v\n‚Ä¢ Operations/sec: %.2f\n\n",
			formatBytes(perf.OplogInfo.Size),
			perf.OplogInfo.TimeWindow,
			perf.OplogInfo.Throughput.OpsPerSecond,
		)
	}

	if perf.ReplicationInfo != nil {
		text += fmt.Sprintf("Replication Info:\n‚Ä¢ Is Replica Set: %t\n‚Ä¢ Is Master: %t\n‚Ä¢ Replica Set: %s\n\n",
			perf.ReplicationInfo.IsReplicaSet,
			perf.ReplicationInfo.IsMaster,
			perf.ReplicationInfo.ReplicaSetName,
		)
	}

	return []*docs.Request{
		{
			InsertText: &docs.InsertTextRequest{
				Location: &docs.Location{Index: -1},
				Text:     text,
			},
		},
	}
}

func formatBytes(bytes int64) string {
	const unit = 1024
	if bytes < unit {
		return fmt.Sprintf("%d B", bytes)
	}
	div, exp := int64(unit), 0
	for n := bytes / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.1f %cB", float64(bytes)/float64(div), "KMGTPE"[exp])
}

// Oplog and Change Stream Analysis Functions

func runOplogAnalysis(ctx context.Context) error {
	fmt.Println("üîç Starting AI-powered oplog analysis...")

	aiProvider, err := createAIProvider(cfg)
	if err != nil {
		return err
	}

	oplogInfo, err := collectOplogInfo(ctx)
	if err != nil {
		return fmt.Errorf("failed to collect oplog information: %w", err)
	}

	replInfo, err := collectReplicationInfo(ctx)
	if err != nil {
		fmt.Printf("Warning: failed to collect replication info: %v\n", err)
	}

	data := map[string]interface{}{
		"oplog":       oplogInfo,
		"replication": replInfo,
	}

	dataJSON, _ := json.MarshalIndent(data, "", "  ")
	prompt := fmt.Sprintf(`Analyze this MongoDB oplog and replication data:

%s

Please provide:
1. Oplog health assessment
2. Replication lag analysis
3. Throughput optimization recommendations
4. Oplog sizing recommendations
5. Replica set configuration advice
6. Monitoring and alerting suggestions

Focus on actionable recommendations for oplog and replication optimization.`, string(dataJSON))

	aiResponse, err := aiProvider.Analyze(ctx, prompt)
	if err != nil {
		return err
	}

	fmt.Printf("\nü§ñ Oplog Analysis (%s)\n", aiProvider.GetProviderName())
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println(aiResponse)

	return nil
}

func runChangeStreamAnalysis(ctx context.Context) error {
	fmt.Println("üîç Starting AI-powered change stream analysis...")

	aiProvider, err := createAIProvider(cfg)
	if err != nil {
		return err
	}

	changeStreams, err := collectChangeStreamInfo(ctx)
	if err != nil {
		return fmt.Errorf("failed to collect change stream information: %w", err)
	}

	changeStreamJSON, _ := json.MarshalIndent(changeStreams, "", "  ")
	prompt := fmt.Sprintf(`Analyze these MongoDB change streams:

%s

Please provide:
1. Change stream configuration assessment
2. Performance optimization recommendations
3. Resume token management advice
4. Real-time processing patterns analysis
5. Error handling and resilience suggestions
6. Scaling recommendations

Focus on best practices for change stream usage and optimization.`, string(changeStreamJSON))

	aiResponse, err := aiProvider.Analyze(ctx, prompt)
	if err != nil {
		return err
	}

	fmt.Printf("\nü§ñ Change Stream Analysis (%s)\n", aiProvider.GetProviderName())
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println(aiResponse)

	return nil
}

func collectOplogInfo(ctx context.Context) (*OplogInfo, error) {
	// Access the local database and oplog collection
	localDB := db.Client().Database("local")
	oplog := localDB.Collection("oplog.rs")

	// Get oplog stats
	statsResult := localDB.RunCommand(ctx, bson.M{"collStats": "oplog.rs"})
	var stats map[string]interface{}
	if err := statsResult.Decode(&stats); err != nil {
		return nil, fmt.Errorf("failed to get oplog stats: %w", err)
	}

	// Get first and last entries
	firstCursor, err := oplog.Find(ctx, bson.M{}, options.Find().SetSort(bson.M{"ts": 1}).SetLimit(1))
	if err != nil {
		return nil, err
	}
	defer firstCursor.Close(ctx)

	lastCursor, err := oplog.Find(ctx, bson.M{}, options.Find().SetSort(bson.M{"ts": -1}).SetLimit(1))
	if err != nil {
		return nil, err
	}
	defer lastCursor.Close(ctx)

	var firstEntry, lastEntry map[string]interface{}
	if firstCursor.Next(ctx) {
		firstCursor.Decode(&firstEntry)
	}
	if lastCursor.Next(ctx) {
		lastCursor.Decode(&lastEntry)
	}

	// Calculate time window
	var timeWindow time.Duration
	if firstEntry != nil && lastEntry != nil {
		// Extract timestamps (simplified - actual implementation would handle BSON timestamps)
		timeWindow = time.Hour * 24 // Placeholder
	}

	// Get operation type distribution
	opTypes := make(map[string]int64)
	nameSpaces := make(map[string]int64)

	// Sample recent operations for analysis
	sampleCursor, _ := oplog.Find(ctx, bson.M{}, options.Find().SetSort(bson.M{"ts": -1}).SetLimit(1000))
	defer sampleCursor.Close(ctx)

	for sampleCursor.Next(ctx) {
		var entry map[string]interface{}
		sampleCursor.Decode(&entry)

		if op, ok := entry["op"].(string); ok {
			opTypes[op]++
		}
		if ns, ok := entry["ns"].(string); ok {
			nameSpaces[ns]++
		}
	}

	info := &OplogInfo{
		TimeWindow:     timeWindow,
		OperationTypes: opTypes,
		Namespaces:     nameSpaces,
		Throughput: OplogThroughput{
			OpsPerSecond: float64(len(opTypes)) / timeWindow.Seconds(), // Simplified calculation
		},
	}

	if size, ok := stats["size"].(float64); ok {
		info.Size = int64(size)
	}
	if maxSize, ok := stats["maxSize"].(float64); ok {
		info.MaxSize = int64(maxSize)
	}
	if count, ok := stats["count"].(float64); ok {
		info.EntryCount = int64(count)
	}

	return info, nil
}

func collectReplicationInfo(ctx context.Context) (*ReplicationInfo, error) {
	// Get replica set status
	replStatusResult := db.RunCommand(ctx, bson.M{"replSetGetStatus": 1})
	var replStatus map[string]interface{}
	if err := replStatusResult.Decode(&replStatus); err != nil {
		// Not a replica set or no access
		return &ReplicationInfo{IsReplicaSet: false}, nil
	}

	// Get is master info
	isMasterResult := db.RunCommand(ctx, bson.M{"isMaster": 1})
	var isMaster map[string]interface{}
	isMasterResult.Decode(&isMaster)

	info := &ReplicationInfo{
		IsReplicaSet: true,
	}

	if setName, ok := replStatus["set"].(string); ok {
		info.ReplicaSetName = setName
	}

	if isMaster, ok := isMaster["ismaster"].(bool); ok {
		info.IsMaster = isMaster
	}

	if isSecondary, ok := isMaster["secondary"].(bool); ok {
		info.IsSecondary = isSecondary
	}

	// Parse members information
	if members, ok := replStatus["members"].([]interface{}); ok {
		for _, member := range members {
			if m, ok := member.(map[string]interface{}); ok {
				memberInfo := ReplicaSetMember{}
				if name, ok := m["name"].(string); ok {
					memberInfo.Host = name
				}
				if state, ok := m["state"].(float64); ok {
					memberInfo.State = int(state)
				}
				if stateStr, ok := m["stateStr"].(string); ok {
					memberInfo.StateStr = stateStr
				}
				if health, ok := m["health"].(float64); ok {
					memberInfo.Health = int(health)
				}
				info.Members = append(info.Members, memberInfo)
			}
		}
	}

	return info, nil
}

func collectChangeStreamInfo(ctx context.Context) ([]ChangeStreamInfo, error) {
	// This is a placeholder implementation
	// In practice, you'd need to check for active change streams
	// which isn't directly queryable through standard MongoDB commands

	// Get collections that might have change streams
	collections, err := db.ListCollectionNames(ctx, bson.M{})
	if err != nil {
		return nil, err
	}

	var changeStreams []ChangeStreamInfo

	// For each collection, we can only provide potential change stream info
	for _, collName := range collections {
		if analyzeCollectionFlag != "" && collName != analyzeCollectionFlag {
			continue
		}

		changeStream := ChangeStreamInfo{
			Namespace:      fmt.Sprintf("%s.%s", cfg.Database, collName),
			Watched:        false,                                  // Can't determine this without application context
			OperationTypes: []string{"insert", "update", "delete"}, // Default operations
		}

		changeStreams = append(changeStreams, changeStream)
	}

	return changeStreams, nil
}
